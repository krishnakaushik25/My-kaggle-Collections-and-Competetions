{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version:  2.3.0\n",
      "Tensorflow version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import transformers\n",
    "from transformers import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "print('Transformers version: ', transformers.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword location  \\\n",
       "2644  3796  destruction      NaN   \n",
       "2227  3185       deluge      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/kaggle/input/nlp-getting-started/'\n",
    "train_df = pd.read_csv(data_dir+'train.csv')\n",
    "test_df = pd.read_csv(data_dir+'test.csv')\n",
    "train_df = train_df.sample(n=len(train_df), random_state=42)\n",
    "sample_submission = pd.read_csv(data_dir+'sample_submission.csv')\n",
    "print(train_df['target'].value_counts())\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "def get_tree_tokens(x):\n",
    "    x = tree_tokenizer.tokenize(x)\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "train_df.text = train_df.text.apply(get_tree_tokens)\n",
    "test_df.text = test_df.text.apply(get_tree_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://www.kaggle.com/utsavnandi/roberta-using-huggingface-tf-implementation\n",
    "def to_tokens(input_text, tokenizer):\n",
    "    output = tokenizer.encode_plus(input_text, max_length=90, pad_to_max_length=True)\n",
    "    return output\n",
    "\n",
    "def select_field(features, field):\n",
    "    return [feature[field] for feature in features]\n",
    "\n",
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    # Removing the @\n",
    "    #tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Removing the URL links\n",
    "    #tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    #tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_data(tokenizer, train_df, test_df):\n",
    "    train_text = train_df['text'].apply(clean_tweet)\n",
    "    test_text = test_df['text'].apply(clean_tweet)\n",
    "    train_encoded = train_text.apply(lambda x: to_tokens(x, tokenizer))\n",
    "    test_encoded = test_text.apply(lambda x: to_tokens(x, tokenizer))\n",
    "\n",
    "    #create attention masks\n",
    "    input_ids_train = np.array(select_field(train_encoded, 'input_ids'))\n",
    "    attention_masks_train = np.array(select_field(train_encoded, 'attention_mask'))\n",
    "\n",
    "    input_ids_test = np.array(select_field(test_encoded, 'input_ids'))\n",
    "    attention_masks_test = np.array(select_field(test_encoded, 'attention_mask'))\n",
    "\n",
    "    # concatonate masks\n",
    "    train_X = [input_ids_train, attention_masks_train]\n",
    "    test_X = [input_ids_test, attention_masks_test]\n",
    "    #OHE target\n",
    "    train_y = tf.keras.utils.to_categorical(train_df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    return train_X, train_y, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/huggingface/transformers\n",
    "# Transformers has a unified API\n",
    "# for 10 transformer architectures and 30 pretrained weights.\n",
    "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
    "def load_pretrained_model(model_class='bert', model_name='bert-base-cased', task='binary', learning_rate=3e-5, epsilon=1e-8, lower_case=False):\n",
    "  MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, TFBertForSequenceClassification, BertTokenizer),\n",
    "    \"xlnet\": (XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer),\n",
    "    \"xlm\": (XLMConfig, TFXLMForSequenceClassification, XLMTokenizer),\n",
    "    \"roberta\": (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "    \"albert\": (AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer),\n",
    "    #\"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer), No tensorflow version yet\n",
    "  }\n",
    "  model_metrics = [\n",
    "        tf.keras.metrics.TruePositives(name='tp'),\n",
    "        tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "  ]\n",
    "\n",
    "  \n",
    "  config_class, model_class, tokenizer_class = MODEL_CLASSES[model_class]\n",
    "\n",
    "  config = config_class.from_pretrained(model_name, num_labels=2, finetuning_task=task)\n",
    "\n",
    "\n",
    "  model = model_class.from_pretrained(model_name)\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=1.0)\n",
    "  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "  metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "  #model.summary()\n",
    "\n",
    "  tokenizer = tokenizer_class.from_pretrained(model_name, lower_case = lower_case)\n",
    "\n",
    "  return config, model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6344 samples, validate on 1269 samples\n",
      "Epoch 1/3\n",
      "6344/6344 [==============================] - 84s 13ms/sample - loss: 0.4952 - accuracy: 0.7418 - val_loss: 0.3883 - val_accuracy: 0.8373\n",
      "Epoch 2/3\n",
      "6344/6344 [==============================] - 64s 10ms/sample - loss: 0.3644 - accuracy: 0.8494 - val_loss: 0.4074 - val_accuracy: 0.8121\n",
      "Epoch 3/3\n",
      "6344/6344 [==============================] - 64s 10ms/sample - loss: 0.3116 - accuracy: 0.8756 - val_loss: 0.3948 - val_accuracy: 0.8420\n",
      "1269/1269 [==============================] - 8s 6ms/sample\n",
      "0.843183609141056\n",
      "3263/3263 [==============================] - 13s 4ms/sample\n",
      "Train on 6344 samples, validate on 1269 samples\n",
      "Epoch 1/3\n",
      "6344/6344 [==============================] - 84s 13ms/sample - loss: 0.4706 - accuracy: 0.7669 - val_loss: 0.4035 - val_accuracy: 0.8160\n",
      "Epoch 2/3\n",
      "6344/6344 [==============================] - 64s 10ms/sample - loss: 0.3557 - accuracy: 0.8521 - val_loss: 0.4172 - val_accuracy: 0.8160\n",
      "Epoch 3/3\n",
      "6344/6344 [==============================] - 64s 10ms/sample - loss: 0.3017 - accuracy: 0.8788 - val_loss: 0.4189 - val_accuracy: 0.8113\n",
      "1269/1269 [==============================] - 8s 7ms/sample\n",
      "0.8156028368794326\n",
      "3263/3263 [==============================] - 13s 4ms/sample\n"
     ]
    }
   ],
   "source": [
    "# load model, process data for model\n",
    "_, _, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\n",
    "train_X, train_y, test_X = preprocess_data(tokenizer=tokenizer, train_df=train_df, test_df=test_df)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=6)\n",
    "test_preds = []\n",
    "i = 0\n",
    "for train_idx, test_idx in kf.split(train_X[0]):\n",
    "    i+=1\n",
    "    if i not in [1, 5]: #only do 2 folds to save time\n",
    "        continue\n",
    "    train_split_X = [train_X[i][train_idx] for i in range(len(train_X))]\n",
    "    test_split_X = [train_X[i][test_idx] for i in range(len(train_X))]\n",
    "\n",
    "    train_split_y = train_y[train_idx]\n",
    "    test_split_y = train_y[test_idx]\n",
    "    #create class weights to account for inbalance\n",
    "    positive = train_df.iloc[train_idx, :].target.value_counts()[0]\n",
    "    negative = train_df.iloc[train_idx, :].target.value_counts()[1]\n",
    "    pos_weight = positive / (positive + negative)\n",
    "    neg_weight = negative / (positive + negative)\n",
    "\n",
    "    class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n",
    "\n",
    "    K.clear_session()\n",
    "    config, model, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\n",
    "\n",
    "    # fit, test model\n",
    "    model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, class_weight=class_weight, validation_data=(test_split_X, test_split_y))\n",
    "\n",
    "    val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n",
    "    val_preds = np.argmax(val_preds, axis=1).flatten()\n",
    "    print(metrics.accuracy_score(train_df.iloc[test_idx, :].target.values, val_preds))\n",
    "\n",
    "    preds1 = model.predict(test_X, batch_size=32, verbose=1)\n",
    "    test_preds.append(preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds2 = np.average(test_preds, axis=0)\n",
    "test_preds3 = np.argmax(test_preds2, axis=1).flatten()\n",
    "sample_submission['target'] = test_preds3\n",
    "sample_submission['target'].value_counts()\n",
    "sample_submission.to_csv('new_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
