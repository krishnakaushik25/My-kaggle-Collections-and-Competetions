{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1156.057848,
      "end_time": "2021-09-02T21:03:53.022314",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-02T20:44:36.964466",
      "version": "2.3.3"
    },
    "colab": {
      "name": "Ensembling-gives-the-best-score.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022545,
          "end_time": "2021-09-02T20:44:44.141778",
          "exception": false,
          "start_time": "2021-09-02T20:44:44.119233",
          "status": "completed"
        },
        "tags": [],
        "id": "cathedral-complement"
      },
      "source": [
        "# 30 Days of ML - Stacked Ensembles\n",
        "\n",
        "\n",
        "I like to think about this solution as \"**aggressive ensembling**.\" It turns out, this approach is very popular in Kaggle because it lets you squeeze as much performance as possible out of your solution.\n",
        "\n",
        "The `Trainer` class and the set of model classes reduced the code footprint considerably and made experimentation much easier.\n"
      ],
      "id": "cathedral-complement"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020223,
          "end_time": "2021-09-02T20:44:44.182458",
          "exception": false,
          "start_time": "2021-09-02T20:44:44.162235",
          "status": "completed"
        },
        "tags": [],
        "id": "transsexual-blink"
      },
      "source": [
        "## Recompiling LGBM with GPU support\n",
        "\n",
        "By default, you can't use an `LGBMRegressor` model with GPU support. \n",
        "\n",
        "For this competition, CPU training is out of the question, so let's recompile LGBM with GPU support."
      ],
      "id": "transsexual-blink"
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2021-09-02T20:44:44.236036Z",
          "iopub.status.busy": "2021-09-02T20:44:44.231529Z",
          "iopub.status.idle": "2021-09-02T20:45:07.818200Z",
          "shell.execute_reply": "2021-09-02T20:45:07.817578Z",
          "shell.execute_reply.started": "2021-09-01T00:31:22.505104Z"
        },
        "papermill": {
          "duration": 23.615587,
          "end_time": "2021-09-02T20:45:07.818415",
          "exception": false,
          "start_time": "2021-09-02T20:44:44.202828",
          "status": "completed"
        },
        "tags": [],
        "id": "bored-suspect"
      },
      "source": [
        "%%capture\n",
        "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "!apt-get install -y -qq libboost-all-dev"
      ],
      "id": "bored-suspect",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:45:07.866366Z",
          "iopub.status.busy": "2021-09-02T20:45:07.862514Z",
          "iopub.status.idle": "2021-09-02T20:49:51.791271Z",
          "shell.execute_reply": "2021-09-02T20:49:51.791744Z",
          "shell.execute_reply.started": "2021-09-01T00:33:00.772278Z"
        },
        "papermill": {
          "duration": 283.953814,
          "end_time": "2021-09-02T20:49:51.791917",
          "exception": false,
          "start_time": "2021-09-02T20:45:07.838103",
          "status": "completed"
        },
        "tags": [],
        "id": "sunset-cleaner"
      },
      "source": [
        "%%capture \n",
        "%%bash\n",
        "cd LightGBM\n",
        "rm -r build\n",
        "mkdir build\n",
        "cd build\n",
        "cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\n",
        "make -j$(nproc)"
      ],
      "id": "sunset-cleaner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:49:51.841048Z",
          "iopub.status.busy": "2021-09-02T20:49:51.839888Z",
          "iopub.status.idle": "2021-09-02T20:49:53.257941Z",
          "shell.execute_reply": "2021-09-02T20:49:53.257404Z",
          "shell.execute_reply.started": "2021-09-01T00:37:44.913381Z"
        },
        "papermill": {
          "duration": 1.445556,
          "end_time": "2021-09-02T20:49:53.258096",
          "exception": false,
          "start_time": "2021-09-02T20:49:51.812540",
          "status": "completed"
        },
        "tags": [],
        "id": "assumed-median"
      },
      "source": [
        "%%capture\n",
        "!cd LightGBM/python-package/;python3 setup.py install --precompile"
      ],
      "id": "assumed-median",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:49:53.309880Z",
          "iopub.status.busy": "2021-09-02T20:49:53.305184Z",
          "iopub.status.idle": "2021-09-02T20:49:54.882215Z",
          "shell.execute_reply": "2021-09-02T20:49:54.880976Z",
          "shell.execute_reply.started": "2021-09-01T00:37:46.466429Z"
        },
        "papermill": {
          "duration": 1.603301,
          "end_time": "2021-09-02T20:49:54.882380",
          "exception": false,
          "start_time": "2021-09-02T20:49:53.279079",
          "status": "completed"
        },
        "tags": [],
        "id": "angry-caution"
      },
      "source": [
        "%%capture\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n",
        "!rm -r LightGBM"
      ],
      "id": "angry-caution",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.019487,
          "end_time": "2021-09-02T20:49:54.921831",
          "exception": false,
          "start_time": "2021-09-02T20:49:54.902344",
          "status": "completed"
        },
        "tags": [],
        "id": "upper-question"
      },
      "source": [
        "## Importing the libraries we need\n",
        "\n",
        "Here is where the competition-specific code begins.\n",
        "\n",
        "Let's import the libraries we need and define a couple of constants that we'll use throughout the notebook."
      ],
      "id": "upper-question"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:49:54.968357Z",
          "iopub.status.busy": "2021-09-02T20:49:54.967551Z",
          "iopub.status.idle": "2021-09-02T20:50:04.684568Z",
          "shell.execute_reply": "2021-09-02T20:50:04.684092Z",
          "shell.execute_reply.started": "2021-09-01T00:37:47.915039Z"
        },
        "papermill": {
          "duration": 9.742858,
          "end_time": "2021-09-02T20:50:04.684698",
          "exception": false,
          "start_time": "2021-09-02T20:49:54.941840",
          "status": "completed"
        },
        "tags": [],
        "id": "north-eating",
        "outputId": "bc76d94b-2049-4447-bd55-5478e91749c4"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import random \n",
        "\n",
        "from sklearn import compose\n",
        "from sklearn import ensemble\n",
        "from sklearn import impute\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn import pipeline\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import lightgbm as lgbm\n",
        "import xgboost as xgb\n",
        "import catboost as cat\n",
        "\n",
        "\n",
        "# This is nice handy constant to turn on and off the GPU. When `False`\n",
        "# the notebook will ignore the GPU even when present.\n",
        "GPU_ENABLED = True\n",
        "\n",
        "if GPU_ENABLED:\n",
        "    # If we want to use the GPU, but we didn't enable it, the code will\n",
        "    # blow up. To make sure this doesn't happen, here I'm checking whether\n",
        "    # we truly have access to a GPU.\n",
        "    from tensorflow.python.client import device_lib\n",
        "    GPU_ENABLED = len(device_lib.list_local_devices()) >= 2\n",
        "\n",
        "# All of the models in this notebook are fitted in a k-fold cross-validation \n",
        "# manner. This constant represents the value of `k`. I got the best results \n",
        "# using 20 folds, but it takes around 8 hours to run this notebook on CPU \n",
        "# using 20 folds, so I set it to 10 here.\n",
        "CROSS_VALIDATION_FOLDS = 10"
      ],
      "id": "north-eating",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type='text/css'>\n",
              ".datatable table.frame { margin-bottom: 0; }\n",
              ".datatable table.frame thead { border-bottom: none; }\n",
              ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
              ".datatable .bool    { background: #DDDD99; }\n",
              ".datatable .object  { background: #565656; }\n",
              ".datatable .int     { background: #5D9E5D; }\n",
              ".datatable .float   { background: #4040CC; }\n",
              ".datatable .str     { background: #CC4040; }\n",
              ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
              ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
              ".datatable th:nth-child(2) { padding-left: 12px; }\n",
              ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
              ".datatable .footer { font-size: 9px; }\n",
              ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.0201,
          "end_time": "2021-09-02T20:50:04.725781",
          "exception": false,
          "start_time": "2021-09-02T20:50:04.705681",
          "status": "completed"
        },
        "tags": [],
        "id": "narrative-hampton"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Let's start by loading the train and test data.\n",
        "\n",
        "We also want to extend the available features by generating dummies for all categorical columns.\n",
        "\n",
        "At the end of this process, we should end up with a dataset with 82 columns."
      ],
      "id": "narrative-hampton"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:04.774814Z",
          "iopub.status.busy": "2021-09-02T20:50:04.774230Z",
          "iopub.status.idle": "2021-09-02T20:50:09.173034Z",
          "shell.execute_reply": "2021-09-02T20:50:09.172569Z",
          "shell.execute_reply.started": "2021-09-01T00:37:57.568241Z"
        },
        "papermill": {
          "duration": 4.427181,
          "end_time": "2021-09-02T20:50:09.173164",
          "exception": false,
          "start_time": "2021-09-02T20:50:04.745983",
          "status": "completed"
        },
        "tags": [],
        "id": "sustainable-george"
      },
      "source": [
        "df_train = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\n",
        "df_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
        "\n",
        "cont_features = [f for f in df_train.columns.tolist() if f.startswith('cont')]\n",
        "cat_features = [f for f in df_train.columns.tolist() if f.startswith('cat')]\n",
        "\n",
        "dummies = pd.get_dummies(df_train.append(df_test)[cat_features])\n",
        "df_train[dummies.columns] = dummies.iloc[:len(df_train), :]\n",
        "df_test[dummies.columns] = dummies.iloc[len(df_train): , :]"
      ],
      "id": "sustainable-george",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020621,
          "end_time": "2021-09-02T20:50:09.215479",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.194858",
          "status": "completed"
        },
        "tags": [],
        "id": "korean-scratch"
      },
      "source": [
        "## Duplicating code sucks\n",
        "\n",
        "It does big time.\n",
        "\n",
        "For this competition, I'll be using several ensemble models stacked on top of each other. I didn't want to duplicate all of the boilerplate code to make that work, so I created a few classes that will help me keep the notebook as clean as possible. \n",
        "\n",
        "Bonus: Experimenting with this mini-framework should be way easier than having to make the exact change all over the notebook.\n",
        "\n",
        "For this idea to work, I first created a wrapper class for each of the four different models I'll be using."
      ],
      "id": "korean-scratch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:09.280192Z",
          "iopub.status.busy": "2021-09-02T20:50:09.278291Z",
          "iopub.status.idle": "2021-09-02T20:50:09.280849Z",
          "shell.execute_reply": "2021-09-02T20:50:09.281265Z",
          "shell.execute_reply.started": "2021-09-01T00:38:01.936646Z"
        },
        "papermill": {
          "duration": 0.045058,
          "end_time": "2021-09-02T20:50:09.281412",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.236354",
          "status": "completed"
        },
        "tags": [],
        "id": "double-leave"
      },
      "source": [
        "class Model(object):\n",
        "    \"\"\"\n",
        "    Base model class from which every specific model implementation will inherit.\n",
        "    Args:\n",
        "        - preprocessor: A standard sklearn pipeline component that will be used\n",
        "            to transform the data.\n",
        "        - model_params: A dictionary with the parameters that will be used\n",
        "            to construct the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, preprocessor=None, model_params={}):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.model_params = model_params\n",
        "        self._model = None\n",
        "\n",
        "    def preprocess(self, datasets):\n",
        "        \"\"\"\n",
        "        Preprocesses the list of supplied datasets using the configured\n",
        "        preprocessor and returns the transformed data.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.preprocessor is None:\n",
        "            return datasets\n",
        "\n",
        "        if datasets is None or len(datasets) == 0:\n",
        "            return []\n",
        "\n",
        "        result = [self.preprocessor.fit_transform(datasets[0])]\n",
        "\n",
        "        for i in range(1, len(datasets)):\n",
        "            result.append(self.preprocessor.transform(datasets[i]))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        \"\"\"\n",
        "        A pass-through function that runs predictions on a dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self._model is None:\n",
        "            return None\n",
        "\n",
        "        return self._model.predict(dataset)\n",
        "\n",
        "\n",
        "class XGBModel(Model):\n",
        "    \"\"\"\n",
        "    A wrapper implementation of an XGBRegressor model.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, preprocessor=None, model_params={}):\n",
        "        super().__init__(preprocessor, model_params)\n",
        "\n",
        "    def fit(self, X_train, y_train, X_valid, y_valid, gpu_enabled=True):\n",
        "        \"\"\"\n",
        "        Fits an instance of the model on the supplied data.\n",
        "        Args:\n",
        "            - X_train: Train data.\n",
        "            - y_train: Train target.\n",
        "            - X_valid: Validation data.\n",
        "            - y_valid: Validation target.\n",
        "            - gpu_enabled: Whether we want to fit the model using the GPU.\n",
        "        \"\"\"\n",
        "        \n",
        "        if gpu_enabled:\n",
        "            self.model_params[\"tree_method\"] = \"gpu_hist\"\n",
        "            self.model_params[\"predictor\"] = \"gpu_predictor\"\n",
        "        else:\n",
        "            self.model_params[\"n_jobs\"] = -1\n",
        "\n",
        "        self._model = xgb.XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            random_state=0, \n",
        "            **self.model_params\n",
        "        ) \n",
        "\n",
        "        self._model.fit(\n",
        "            X_train, \n",
        "            y_train, \n",
        "            early_stopping_rounds=300, \n",
        "            eval_set=[(X_valid, y_valid)], \n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        return self._model\n",
        "\n",
        "\n",
        "class LGBMModel(Model):\n",
        "    \"\"\"\n",
        "    A wrapper implementation of an LGBMRegressor model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, preprocessor=None, model_params={}):\n",
        "        super().__init__(preprocessor, model_params)\n",
        "\n",
        "    def fit(self, X_train, y_train, X_valid, y_valid, gpu_enabled=True):\n",
        "        \"\"\"\n",
        "        Fits an instance of the model on the supplied data.\n",
        "        Args:\n",
        "            - X_train: Train data.\n",
        "            - y_train: Train target.\n",
        "            - X_valid: Validation data.\n",
        "            - y_valid: Validation target.\n",
        "            - gpu_enabled: Whether we want to fit the model using the GPU.\n",
        "        \"\"\"\n",
        "        \n",
        "        if gpu_enabled:\n",
        "            self.model_params[\"device\"] = \"gpu\"\n",
        "            self.model_params[\"gpu_platform_id\"] = 0\n",
        "            self.model_params[\"gpu_device_id\"] = 0\n",
        "        else:\n",
        "            self.model_params[\"n_jobs\"] = -1\n",
        "\n",
        "        self._model = lgbm.LGBMRegressor(\n",
        "            objective='regression', \n",
        "            metric=\"rmse\",\n",
        "            random_state=0,\n",
        "            **self.model_params\n",
        "        )\n",
        "\n",
        "        self._model.fit(\n",
        "            X_train, \n",
        "            y_train, \n",
        "            early_stopping_rounds=300, \n",
        "            eval_set=[(X_valid, y_valid)], \n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        return self._model\n",
        "\n",
        "\n",
        "class CatBoostModel(Model):\n",
        "    \"\"\"\n",
        "    A wrapper implementation of a CatBoostRegressor model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, preprocessor=None, model_params={}):\n",
        "        super().__init__(preprocessor, model_params)\n",
        "\n",
        "    def fit(self, X_train, y_train, X_valid, y_valid, gpu_enabled=True):\n",
        "        \"\"\"\n",
        "        Fits an instance of the model on the supplied data.\n",
        "        Args:\n",
        "            - X_train: Train data.\n",
        "            - y_train: Train target.\n",
        "            - X_valid: Validation data.\n",
        "            - y_valid: Validation target.\n",
        "            - gpu_enabled: Whether we want to fit the model using the GPU.\n",
        "        \"\"\"\n",
        "        \n",
        "        if gpu_enabled:\n",
        "            self.model_params[\"task_type\"] = \"GPU\"\n",
        "            self.model_params[\"bootstrap_type\"] = \"Poisson\"\n",
        "\n",
        "        self._model = cat.CatBoostRegressor(\n",
        "            loss_function='RMSE', \n",
        "            random_state=0,\n",
        "            **self.model_params\n",
        "        )\n",
        "\n",
        "        self._model.fit(\n",
        "            X_train, \n",
        "            y_train, \n",
        "            early_stopping_rounds=300, \n",
        "            eval_set=[(X_valid, y_valid)], \n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        return self._model\n",
        "\n",
        "    \n",
        "class LassoModel(Model):\n",
        "    \"\"\"\n",
        "    A wrapper implementation of a Lasso model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, preprocessor=None, model_params={}):\n",
        "        super().__init__(preprocessor, model_params)\n",
        "\n",
        "    def fit(self, X_train, y_train, **kwargs):\n",
        "        \"\"\"\n",
        "        Fits an instance of the model on the supplied data.\n",
        "        Args:\n",
        "            - X_train: Train data.\n",
        "            - y_train: Train target.\n",
        "        \"\"\"\n",
        "        \n",
        "        self._model = linear_model.Lasso(\n",
        "            precompute=True,\n",
        "            positive=True, \n",
        "            random_state=999, \n",
        "            fit_intercept=True,\n",
        "            **self.model_params\n",
        "        )\n",
        "\n",
        "        self._model.fit(\n",
        "            X_train, \n",
        "            y_train\n",
        "        )\n",
        "\n",
        "        return self._model\n",
        "    "
      ],
      "id": "double-leave",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020303,
          "end_time": "2021-09-02T20:50:09.322389",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.302086",
          "status": "completed"
        },
        "tags": [],
        "id": "going-memorabilia"
      },
      "source": [
        "Let's now define the class where the actual exciting stuff happens.\n",
        "\n",
        "The `Trainer` class is the one I'll use to train all of the models. It encapsulates the k-fold cross-validation plumbing, predictions, and metrics."
      ],
      "id": "going-memorabilia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:09.383979Z",
          "iopub.status.busy": "2021-09-02T20:50:09.382068Z",
          "iopub.status.idle": "2021-09-02T20:50:09.384855Z",
          "shell.execute_reply": "2021-09-02T20:50:09.385372Z",
          "shell.execute_reply.started": "2021-09-01T00:38:01.96223Z"
        },
        "papermill": {
          "duration": 0.042465,
          "end_time": "2021-09-02T20:50:09.385543",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.343078",
          "status": "completed"
        },
        "tags": [],
        "id": "strategic-terrorist"
      },
      "source": [
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Builds an ensemble of models using k-fold cross-validation.\n",
        "    \n",
        "    Args:\n",
        "        - df_train: Pandas dataframe containing the train data.\n",
        "        - df_test: Pandas dataframe containing the test data.\n",
        "        - folds: How many folds will be used for the k-fold cross-validation\n",
        "            process.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, df_train, df_test, folds=10):\n",
        "        self.df_train = df_train\n",
        "        self.df_test = df_test\n",
        "        self.folds = folds\n",
        "\n",
        "        self.predictions_valid = None\n",
        "        self.predictions_test = None\n",
        "\n",
        "    def fit(self, models, gpu_enabled=True):\n",
        "        \"\"\"\n",
        "        Fits each one of the supplied models in a k-fold cross-validation manner.\n",
        "        Args:\n",
        "            - models: The list of models that will be fitted.\n",
        "            - gpu_enabled: Whether we want to fit the models using the GPU.\n",
        "        Returns:\n",
        "            The list of MSE scores corresponding to each one of the \n",
        "            supplied models.\n",
        "        \"\"\"\n",
        "        \n",
        "        model_scores = dict()\n",
        "        self.predictions_valid = dict()\n",
        "        self.predictions_test = dict()\n",
        "\n",
        "        for model_index in range(len(models)):\n",
        "            model_scores.setdefault(model_index, [])\n",
        "            self.predictions_valid.setdefault(model_index, dict())\n",
        "            self.predictions_test.setdefault(model_index, [])\n",
        "\n",
        "        self.preprocessed_data = dict()\n",
        "        \n",
        "        kfold = model_selection.KFold(\n",
        "            n_splits=self.folds, \n",
        "            shuffle=True, \n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        for fold, (train_idx, valid_idx) in enumerate(kfold.split(self.df_train)):\n",
        "            X_train, X_valid = self.df_train.iloc[train_idx], self.df_train.iloc[valid_idx]\n",
        "\n",
        "            y_train = X_train.target\n",
        "            y_valid = X_valid.target\n",
        "\n",
        "            X_train = X_train.drop([\"target\"], axis=1)\n",
        "            X_valid = X_valid.drop([\"target\"], axis=1)\n",
        "\n",
        "            for model_index, model in enumerate(models):\n",
        "                [xtrain, xvalid, xtest] = model.preprocess([\n",
        "                    X_train, \n",
        "                    X_valid, \n",
        "                    self.df_test\n",
        "                ])\n",
        "\n",
        "                model.fit(\n",
        "                    xtrain, \n",
        "                    y_train, \n",
        "                    X_valid=xvalid, \n",
        "                    y_valid=y_valid,\n",
        "                    gpu_enabled=gpu_enabled\n",
        "                )\n",
        "\n",
        "                yhat_valid = model.predict(xvalid)\n",
        "                yhat_test = model.predict(xtest)\n",
        "\n",
        "                valid_ids = X_valid.id.values.tolist()\n",
        "                self.predictions_valid[model_index].update(\n",
        "                    dict(zip(valid_ids, yhat_valid))\n",
        "                )\n",
        "\n",
        "                self.predictions_test[model_index].append(yhat_test)\n",
        "\n",
        "                rmse = metrics.mean_squared_error(y_valid, yhat_valid, squared=False)\n",
        "                model_scores[model_index].append(rmse)\n",
        "\n",
        "                print(f\"[FOLD {fold}] Model {model_index + 1} Score: {rmse}\")\n",
        "\n",
        "        print()\n",
        "        scores = []\n",
        "        for index in range(len(models)):\n",
        "            score = np.mean(model_scores[index])\n",
        "            scores.append(score)\n",
        "            print(f\"Model {index} Overall Score: {score}\")\n",
        "\n",
        "        return scores\n",
        "    \n",
        "    def get_prediction_data(self):\n",
        "        \"\"\"\n",
        "        Returns two new Pandas datasets: A train dataset containing \n",
        "        the predictions on the validation data, and a test dataset containing \n",
        "        the predictions on the test data.\n",
        "        \n",
        "        For example, if we fit 3 models, the output datasets will contain a \n",
        "        column with the predictions generated with each one of these models. \n",
        "        The columns will be named consecutively, i.e. `prediction_0` (predictions \n",
        "        generated by the first model,) `prediction_1` (predictions generated\n",
        "        by the second model,) and `prediction_2` (predictions generated by the \n",
        "        third model.)\n",
        "        \n",
        "        Both the `id` and `target` columns will be included in the resultant \n",
        "        train dataset. The `id` column will also be included in the resultant \n",
        "        `test` dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.predictions_valid is None:\n",
        "            return None\n",
        "        \n",
        "        df_train_predictions = self.df_train[[\"id\", \"target\"]]\n",
        "        df_test_predictions = self.df_test[[\"id\"]]\n",
        "\n",
        "        number_of_trained_models = len(self.predictions_valid.keys())\n",
        "        for index in range(number_of_trained_models):\n",
        "            df_predictions_valid = pd.DataFrame.from_dict(\n",
        "                self.predictions_valid[index], orient=\"index\"\n",
        "            ).reset_index()\n",
        "            df_predictions_valid.columns = [\"id\", f\"prediction_{index}\"]\n",
        "\n",
        "            df_predictions_test = pd.DataFrame({\n",
        "                \"id\": self.df_test.id,\n",
        "                f\"prediction_{index}\": np.mean(\n",
        "                    np.column_stack(self.predictions_test[index]\n",
        "                ), axis=1)\n",
        "            })\n",
        "\n",
        "            df_train_predictions = df_train_predictions.merge(\n",
        "                df_predictions_valid, on=\"id\", how=\"left\"\n",
        "            )\n",
        "            df_test_predictions = df_test_predictions.merge(\n",
        "                df_predictions_test, on=\"id\", how=\"left\"\n",
        "            )\n",
        "\n",
        "        return df_train_predictions, df_test_predictions"
      ],
      "id": "strategic-terrorist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.025225,
          "end_time": "2021-09-02T20:50:09.432707",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.407482",
          "status": "completed"
        },
        "tags": [],
        "id": "guided-polymer"
      },
      "source": [
        "## Preprocessing pipeline\n",
        "\n",
        "Let's define the preprocessing transformations that I will use with the original data.\n",
        "\n",
        "Notice that there are only three different transformations that I will be using:\n",
        "\n",
        "1. Scaling values using a Min-Max Scaler.\n",
        "2. Transforming categorical columns to ordinal values.\n",
        "3. One-hot encoding categorical columns.\n",
        "\n",
        "I combine all of this later to preprocess the data in different ways."
      ],
      "id": "guided-polymer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:09.487776Z",
          "iopub.status.busy": "2021-09-02T20:50:09.486935Z",
          "iopub.status.idle": "2021-09-02T20:50:09.491166Z",
          "shell.execute_reply": "2021-09-02T20:50:09.490686Z",
          "shell.execute_reply.started": "2021-08-29T13:44:19.548243Z"
        },
        "papermill": {
          "duration": 0.034548,
          "end_time": "2021-09-02T20:50:09.491319",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.456771",
          "status": "completed"
        },
        "tags": [],
        "id": "hundred-drink"
      },
      "source": [
        "numerical_preprocessor = pipeline.Pipeline(steps=[\n",
        "    (\"imputer\", impute.SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", preprocessing.MinMaxScaler()),\n",
        "])\n",
        "\n",
        "categorical_preprocessor = pipeline.Pipeline(steps=[\n",
        "    (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ordinal\", preprocessing.OrdinalEncoder()),\n",
        "])\n",
        "\n",
        "onehot_preprocessor = preprocessing.OneHotEncoder(handle_unknown=\"ignore\")"
      ],
      "id": "hundred-drink",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02111,
          "end_time": "2021-09-02T20:50:09.536473",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.515363",
          "status": "completed"
        },
        "tags": [],
        "id": "ceramic-leave"
      },
      "source": [
        "There are three different preprocessors here that I will use to transform the data for the first three different models:\n",
        "\n",
        "1. `preprocessor1`: Scales numerical columns and converts categorical columns to ordinal values.\n",
        "2. `preprocessor2`: Scales numerical columns, converts low-order categorical columns to ordinal values, and one-hot encodes high-order categorical columns.\n",
        "3. `preprocessor3`: Scales numerical columns, converts some of the categorical columns to ordinal values and keeps a list of dummy-generated columns untouched.\n",
        "\n",
        "Each one of these preprocessors will be used together with one of the models of the first ensemble.\n",
        "\n",
        "By the way, I tried many different transformations. In the end, these were the ones that gave me the best results."
      ],
      "id": "ceramic-leave"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:09.584233Z",
          "iopub.status.busy": "2021-09-02T20:50:09.583666Z",
          "iopub.status.idle": "2021-09-02T20:50:09.587894Z",
          "shell.execute_reply": "2021-09-02T20:50:09.587458Z",
          "shell.execute_reply.started": "2021-08-29T13:44:19.561796Z"
        },
        "papermill": {
          "duration": 0.031125,
          "end_time": "2021-09-02T20:50:09.588009",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.556884",
          "status": "completed"
        },
        "tags": [],
        "id": "vocational-algorithm"
      },
      "source": [
        "preprocessor1 = compose.ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numerical\", numerical_preprocessor, cont_features),\n",
        "        (\"categorical\", categorical_preprocessor, cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor2 = compose.ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numerical\", numerical_preprocessor, cont_features),\n",
        "        (\"categorical\", categorical_preprocessor, [\n",
        "            \"cat0\", \"cat1\", \"cat2\", \"cat6\", \"cat7\", \"cat8\", \"cat9\"\n",
        "        ]),\n",
        "        (\"onehot\", onehot_preprocessor, [\"cat3\", \"cat4\", \"cat5\"])\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor3 = compose.ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numerical\", numerical_preprocessor, cont_features),\n",
        "        (\"categorical\", categorical_preprocessor, [\"cat1\", \"cat5\", \"cat8\"]),\n",
        "        (\"passthrough\", \"passthrough\", [\"cat1_A\", \"cat3_C\", \"cat8_C\", \"cat8_E\"])\n",
        "    ]\n",
        ")"
      ],
      "id": "vocational-algorithm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020617,
          "end_time": "2021-09-02T20:50:09.628953",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.608336",
          "status": "completed"
        },
        "tags": [],
        "id": "accepting-zambia"
      },
      "source": [
        "## Building the first ensemble\n",
        "\n",
        "I decided to build three different models:\n",
        "\n",
        "1. An `XGBRegressor` paired with `preprocessor1`.\n",
        "1. An `XGBRegressor` paired with `preprocessor2`.\n",
        "1. An `XGBRegressor` paired with `preprocessor3`.\n",
        "\n",
        "Since each model uses slightly different data, they should learn different things that we can later put together in a more robust model. Instead of using three `XGBRegressor` models, a better approach would've been using three completely different models. In this case, since the data is different for each model, the three `XGBRegressor` models do an excellent job of approaching the problem differently.\n",
        "\n",
        "I also experimented with an additional `LGBMRegressor` and a `CatBoostRegressor` for a total of five models. After a lot of back and forth, the solution with only the three `XGBRegressor` models was much better, so I got rid of the other models.\n",
        "\n",
        "I tuned the hyperparameters of each one of these models in a separate notebook."
      ],
      "id": "accepting-zambia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:09.677617Z",
          "iopub.status.busy": "2021-09-02T20:50:09.676905Z",
          "iopub.status.idle": "2021-09-02T20:50:09.680750Z",
          "shell.execute_reply": "2021-09-02T20:50:09.680322Z",
          "shell.execute_reply.started": "2021-08-29T13:44:19.571978Z"
        },
        "papermill": {
          "duration": 0.031189,
          "end_time": "2021-09-02T20:50:09.680869",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.649680",
          "status": "completed"
        },
        "tags": [],
        "id": "continued-bookmark"
      },
      "source": [
        "ensemble1_model1 = XGBModel(preprocessor1, model_params = {\n",
        "    'n_estimators': 9609,\n",
        "    'colsample_bytree': 0.1023603386994367,\n",
        "    'learning_rate': 0.05058244812315823,\n",
        "    'max_depth': 4,\n",
        "    'reg_alpha': 52.62163397880715,\n",
        "    'reg_lambda': 0.09796566981609614,\n",
        "    'subsample': 0.9362560643601167\n",
        "})\n",
        "\n",
        "\n",
        "ensemble1_model2 = XGBModel(preprocessor2, model_params = {\n",
        "    'n_estimators': 7128,\n",
        "    'reg_alpha': 19.910244642239753,\n",
        "    'reg_lambda': 1.8655469044829698,\n",
        "    'subsample': 0.7843892199651581,\n",
        "    'learning_rate': 0.0315114451657133,\n",
        "    'max_depth': 5,\n",
        "    'colsample_bytree': 0.103069950932443\n",
        "})\n",
        "\n",
        "\n",
        "ensemble1_model3 = XGBModel(preprocessor3, model_params = {\n",
        "    'n_estimators': 7144,\n",
        "    'reg_alpha': 3.9027631496250404e-05,\n",
        "    'reg_lambda': 38.44801773583271,\n",
        "    'subsample': 0.6,\n",
        "    'learning_rate': 0.04150001273205032,\n",
        "    'max_depth': 3,\n",
        "    'colsample_bytree': 0.11939190672649105\n",
        "})"
      ],
      "id": "continued-bookmark",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020319,
          "end_time": "2021-09-02T20:50:09.721865",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.701546",
          "status": "completed"
        },
        "tags": [],
        "id": "closing-retreat"
      },
      "source": [
        "Now that every model of the first ensemble is defined, we can use the `Trainer` class to train them in a cross-validated manner."
      ],
      "id": "closing-retreat"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:50:09.769009Z",
          "iopub.status.busy": "2021-09-02T20:50:09.768122Z",
          "iopub.status.idle": "2021-09-02T20:58:57.266901Z",
          "shell.execute_reply": "2021-09-02T20:58:57.267389Z",
          "shell.execute_reply.started": "2021-08-29T13:44:19.591412Z"
        },
        "papermill": {
          "duration": 527.524314,
          "end_time": "2021-09-02T20:58:57.267554",
          "exception": false,
          "start_time": "2021-09-02T20:50:09.743240",
          "status": "completed"
        },
        "tags": [],
        "id": "false-confirmation",
        "outputId": "2cb88784-cbb9-429a-df37-39c47263e6ae"
      },
      "source": [
        "ensemble1 = Trainer(df_train, df_test, folds=CROSS_VALIDATION_FOLDS)\n",
        "ensemble1.fit([ensemble1_model1, ensemble1_model2, ensemble1_model3], GPU_ENABLED)"
      ],
      "id": "false-confirmation",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FOLD 0] Model 1 Score: 0.7170723419448163\n",
            "[FOLD 0] Model 2 Score: 0.716975951972156\n",
            "[FOLD 0] Model 3 Score: 0.7173449693087539\n",
            "[FOLD 1] Model 1 Score: 0.7167072315356829\n",
            "[FOLD 1] Model 2 Score: 0.7165407076990261\n",
            "[FOLD 1] Model 3 Score: 0.7171438385121012\n",
            "[FOLD 2] Model 1 Score: 0.7160510948987522\n",
            "[FOLD 2] Model 2 Score: 0.7158262945427482\n",
            "[FOLD 2] Model 3 Score: 0.7160312657278953\n",
            "[FOLD 3] Model 1 Score: 0.7179011964034449\n",
            "[FOLD 3] Model 2 Score: 0.7179614277501329\n",
            "[FOLD 3] Model 3 Score: 0.718187265874558\n",
            "[FOLD 4] Model 1 Score: 0.7219106390797846\n",
            "[FOLD 4] Model 2 Score: 0.7218587912247788\n",
            "[FOLD 4] Model 3 Score: 0.7222953960855346\n",
            "[FOLD 5] Model 1 Score: 0.7152015037498958\n",
            "[FOLD 5] Model 2 Score: 0.7149304408745646\n",
            "[FOLD 5] Model 3 Score: 0.7153699512878776\n",
            "[FOLD 6] Model 1 Score: 0.7180949884791823\n",
            "[FOLD 6] Model 2 Score: 0.7178412762887201\n",
            "[FOLD 6] Model 3 Score: 0.7184699004006689\n",
            "[FOLD 7] Model 1 Score: 0.7188960203142062\n",
            "[FOLD 7] Model 2 Score: 0.7188102517738338\n",
            "[FOLD 7] Model 3 Score: 0.7192055003056848\n",
            "[FOLD 8] Model 1 Score: 0.7203633157920268\n",
            "[FOLD 8] Model 2 Score: 0.7202919730470285\n",
            "[FOLD 8] Model 3 Score: 0.7203619299016851\n",
            "[FOLD 9] Model 1 Score: 0.7134929686671045\n",
            "[FOLD 9] Model 2 Score: 0.7135738951845068\n",
            "[FOLD 9] Model 3 Score: 0.7134798000166681\n",
            "\n",
            "Model 0 Overall Score: 0.7175691300864897\n",
            "Model 1 Overall Score: 0.7174611010357497\n",
            "Model 2 Overall Score: 0.7177889817421428\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7175691300864897, 0.7174611010357497, 0.7177889817421428]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.028748,
          "end_time": "2021-09-02T20:58:57.326645",
          "exception": false,
          "start_time": "2021-09-02T20:58:57.297897",
          "status": "completed"
        },
        "tags": [],
        "id": "excited-easter"
      },
      "source": [
        "## Building the second ensemble\n",
        "\n",
        "At this point, we have the results of the first ensemble of models. We are now going to stack a second ensemble on top of it. The models of this second ensemble will use the predicted results of the first set of models as the input features. \n",
        "\n",
        "Let's prepare this new dataset:\n",
        "\n",
        "1. The training dataset contains the predictions of the first set of models on the validation data.\n",
        "2. The test dataset contains the predictions of the first set of models on the test data.\n",
        "\n",
        "The implementation of the `get_prediction_data()` function combines the results of the ensemble models into the format we need."
      ],
      "id": "excited-easter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:58:57.402146Z",
          "iopub.status.busy": "2021-09-02T20:58:57.401355Z",
          "iopub.status.idle": "2021-09-02T20:58:58.234972Z",
          "shell.execute_reply": "2021-09-02T20:58:58.235426Z",
          "shell.execute_reply.started": "2021-08-29T13:48:31.561902Z"
        },
        "papermill": {
          "duration": 0.879969,
          "end_time": "2021-09-02T20:58:58.235576",
          "exception": false,
          "start_time": "2021-09-02T20:58:57.355607",
          "status": "completed"
        },
        "tags": [],
        "id": "muslim-apparel",
        "outputId": "a795d68a-40f6-43c0-faa7-08741395afc1"
      },
      "source": [
        "df_train_ensemble1, df_test_ensemble1 = ensemble1.get_prediction_data()\n",
        "df_train_ensemble1"
      ],
      "id": "muslim-apparel",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>prediction_0</th>\n",
              "      <th>prediction_1</th>\n",
              "      <th>prediction_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>8.113634</td>\n",
              "      <td>8.412793</td>\n",
              "      <td>8.469507</td>\n",
              "      <td>8.464657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>8.481233</td>\n",
              "      <td>8.359716</td>\n",
              "      <td>8.385630</td>\n",
              "      <td>8.396006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>8.364351</td>\n",
              "      <td>8.206654</td>\n",
              "      <td>8.201128</td>\n",
              "      <td>8.187977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>8.049253</td>\n",
              "      <td>8.412392</td>\n",
              "      <td>8.399385</td>\n",
              "      <td>8.374225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>7.972260</td>\n",
              "      <td>8.187385</td>\n",
              "      <td>8.206850</td>\n",
              "      <td>8.247770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299995</th>\n",
              "      <td>499993</td>\n",
              "      <td>7.945605</td>\n",
              "      <td>8.300077</td>\n",
              "      <td>8.306719</td>\n",
              "      <td>8.319247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299996</th>\n",
              "      <td>499996</td>\n",
              "      <td>7.326118</td>\n",
              "      <td>7.723757</td>\n",
              "      <td>7.708890</td>\n",
              "      <td>7.705111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299997</th>\n",
              "      <td>499997</td>\n",
              "      <td>8.706755</td>\n",
              "      <td>8.248857</td>\n",
              "      <td>8.245758</td>\n",
              "      <td>8.250740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299998</th>\n",
              "      <td>499998</td>\n",
              "      <td>7.229569</td>\n",
              "      <td>7.954183</td>\n",
              "      <td>7.947504</td>\n",
              "      <td>7.987178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299999</th>\n",
              "      <td>499999</td>\n",
              "      <td>8.631146</td>\n",
              "      <td>8.276460</td>\n",
              "      <td>8.319151</td>\n",
              "      <td>8.291594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300000 rows  5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id    target  prediction_0  prediction_1  prediction_2\n",
              "0            1  8.113634      8.412793      8.469507      8.464657\n",
              "1            2  8.481233      8.359716      8.385630      8.396006\n",
              "2            3  8.364351      8.206654      8.201128      8.187977\n",
              "3            4  8.049253      8.412392      8.399385      8.374225\n",
              "4            6  7.972260      8.187385      8.206850      8.247770\n",
              "...        ...       ...           ...           ...           ...\n",
              "299995  499993  7.945605      8.300077      8.306719      8.319247\n",
              "299996  499996  7.326118      7.723757      7.708890      7.705111\n",
              "299997  499997  8.706755      8.248857      8.245758      8.250740\n",
              "299998  499998  7.229569      7.954183      7.947504      7.987178\n",
              "299999  499999  8.631146      8.276460      8.319151      8.291594\n",
              "\n",
              "[300000 rows x 5 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.028618,
          "end_time": "2021-09-02T20:58:58.293317",
          "exception": false,
          "start_time": "2021-09-02T20:58:58.264699",
          "status": "completed"
        },
        "tags": [],
        "id": "hollywood-retention"
      },
      "source": [
        "We don't need to do any data transformation for this second ensemble because we are working with numerical columns with similar characteristics. \n",
        "\n",
        "The `passthrough_preprocessor` transformer lets the relevant columns pass through (every column starting with `prediction_`) and drops the columns we don't need (like `id` and `target`.)"
      ],
      "id": "hollywood-retention"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:58:58.356615Z",
          "iopub.status.busy": "2021-09-02T20:58:58.355208Z",
          "iopub.status.idle": "2021-09-02T20:58:58.357915Z",
          "shell.execute_reply": "2021-09-02T20:58:58.357522Z",
          "shell.execute_reply.started": "2021-08-29T13:48:32.363517Z"
        },
        "papermill": {
          "duration": 0.035996,
          "end_time": "2021-09-02T20:58:58.358048",
          "exception": false,
          "start_time": "2021-09-02T20:58:58.322052",
          "status": "completed"
        },
        "tags": [],
        "id": "editorial-honolulu"
      },
      "source": [
        "passthrough_preprocessor = compose.ColumnTransformer(\n",
        "    transformers=[(\n",
        "        \"passthrough\", \n",
        "        \"passthrough\", \n",
        "        [c for c in df_train_ensemble1.columns.tolist() if c.startswith(\"prediction\")]\n",
        "    )]\n",
        ")"
      ],
      "id": "editorial-honolulu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.028281,
          "end_time": "2021-09-02T20:58:58.415144",
          "exception": false,
          "start_time": "2021-09-02T20:58:58.386863",
          "status": "completed"
        },
        "tags": [],
        "id": "marine-drinking"
      },
      "source": [
        "For this second ensemble, I built another three models:\n",
        "\n",
        "1. An `XGBRegressor`.\n",
        "1. An `LGBMRegressor`.\n",
        "1. A `CatBoostRegressor`.\n",
        "\n",
        "In this case, these models will be looking at the exact same data, but their characteristics are different, so hopefully, they'll learn different patterns from the data.\n",
        "\n",
        "I tuned the hyperparameters of each one of these models in a separate notebook.\n",
        "\n",
        "Finally, we can also fit all three models using the `Trainer` class. (Here is where the `Trainer` class starts paying off because we don't need to duplicate all of that code.)"
      ],
      "id": "marine-drinking"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T20:58:58.481084Z",
          "iopub.status.busy": "2021-09-02T20:58:58.480346Z",
          "iopub.status.idle": "2021-09-02T21:03:47.663957Z",
          "shell.execute_reply": "2021-09-02T21:03:47.664960Z",
          "shell.execute_reply.started": "2021-08-29T13:48:32.370742Z"
        },
        "papermill": {
          "duration": 289.221248,
          "end_time": "2021-09-02T21:03:47.665179",
          "exception": false,
          "start_time": "2021-09-02T20:58:58.443931",
          "status": "completed"
        },
        "tags": [],
        "id": "commercial-observer",
        "outputId": "88c96ae7-7494-4e5a-8fde-71bd9443412c"
      },
      "source": [
        "ensemble2_model1 = XGBModel(passthrough_preprocessor, model_params={\n",
        "    'n_estimators': 10911,\n",
        "    'reg_alpha': 1.1662382319696787e-08,\n",
        "    'reg_lambda': 18.709461290330285,\n",
        "    'subsample': 0.5,\n",
        "    'learning_rate': 0.02841601836601206,\n",
        "    'max_depth': 2,\n",
        "    'colsample_bytree': 0.6137187091045387\n",
        "})\n",
        "\n",
        "\n",
        "ensemble2_model2 = LGBMModel(passthrough_preprocessor, model_params = {\n",
        "    'n_estimators': 5000,\n",
        "    'learning_rate': 0.021040088115256594,\n",
        "    'max_depth': 1,\n",
        "    'num_leaves': 136,\n",
        "    'min_child_samples': 52\n",
        "})\n",
        "\n",
        "\n",
        "ensemble2_model3 = CatBoostModel(passthrough_preprocessor, model_params = {\n",
        "    'iterations': 20969,\n",
        "    'od_wait': 2248,\n",
        "    'learning_rate': 0.010572354868717486,\n",
        "    'reg_lambda': 27.522864565371602,\n",
        "    'subsample': 0.022762417549602867,\n",
        "    'random_strength': 30.635308112394423,\n",
        "    'depth': 1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'leaf_estimation_iterations': 3\n",
        "})\n",
        "\n",
        "\n",
        "ensemble2 = Trainer(\n",
        "    df_train_ensemble1, \n",
        "    df_test_ensemble1, \n",
        "    folds=CROSS_VALIDATION_FOLDS\n",
        ")\n",
        "ensemble2.fit([ensemble2_model1, ensemble2_model2, ensemble2_model3], GPU_ENABLED)"
      ],
      "id": "commercial-observer",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FOLD 0] Model 1 Score: 0.7170621976081205\n",
            "[FOLD 0] Model 2 Score: 0.7169762504315812\n",
            "[FOLD 0] Model 3 Score: 0.7169394905208561\n",
            "[FOLD 1] Model 1 Score: 0.7163572833101155\n",
            "[FOLD 1] Model 2 Score: 0.7164912442891678\n",
            "[FOLD 1] Model 3 Score: 0.7163359144475072\n",
            "[FOLD 2] Model 1 Score: 0.7157173922945768\n",
            "[FOLD 2] Model 2 Score: 0.7156887678267895\n",
            "[FOLD 2] Model 3 Score: 0.7156942801086544\n",
            "[FOLD 3] Model 1 Score: 0.7175521111494717\n",
            "[FOLD 3] Model 2 Score: 0.7175668039182104\n",
            "[FOLD 3] Model 3 Score: 0.7175460447000896\n",
            "[FOLD 4] Model 1 Score: 0.7217543165494482\n",
            "[FOLD 4] Model 2 Score: 0.7218103241826395\n",
            "[FOLD 4] Model 3 Score: 0.7217329428078125\n",
            "[FOLD 5] Model 1 Score: 0.7147483726458623\n",
            "[FOLD 5] Model 2 Score: 0.7146625758590391\n",
            "[FOLD 5] Model 3 Score: 0.7146350497126678\n",
            "[FOLD 6] Model 1 Score: 0.7175948238586292\n",
            "[FOLD 6] Model 2 Score: 0.7175494118663038\n",
            "[FOLD 6] Model 3 Score: 0.717578598693998\n",
            "[FOLD 7] Model 1 Score: 0.7189303275028172\n",
            "[FOLD 7] Model 2 Score: 0.7188244169186816\n",
            "[FOLD 7] Model 3 Score: 0.7188919427079089\n",
            "[FOLD 8] Model 1 Score: 0.7198663955809425\n",
            "[FOLD 8] Model 2 Score: 0.7199537565341874\n",
            "[FOLD 8] Model 3 Score: 0.719721770863928\n",
            "[FOLD 9] Model 1 Score: 0.713072514785715\n",
            "[FOLD 9] Model 2 Score: 0.7131747458411557\n",
            "[FOLD 9] Model 3 Score: 0.7130471642712097\n",
            "\n",
            "Model 0 Overall Score: 0.7172655735285699\n",
            "Model 1 Overall Score: 0.7172698297667756\n",
            "Model 2 Overall Score: 0.7172123198834632\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7172655735285699, 0.7172698297667756, 0.7172123198834632]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.054506,
          "end_time": "2021-09-02T21:03:47.779399",
          "exception": false,
          "start_time": "2021-09-02T21:03:47.724893",
          "status": "completed"
        },
        "tags": [],
        "id": "theoretical-inspection"
      },
      "source": [
        "## Building the final model\n",
        "\n",
        "We need one more model to produce the final results.\n",
        "\n",
        "We will stack this model on top of the second ensemble, and its goal will be to combine the predictions of each model into a final prediction.\n",
        "\n",
        "As before, we will use the data produced by the second ensemble to train and evaluate this final model."
      ],
      "id": "theoretical-inspection"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T21:03:47.905133Z",
          "iopub.status.busy": "2021-09-02T21:03:47.904440Z",
          "iopub.status.idle": "2021-09-02T21:03:48.847121Z",
          "shell.execute_reply": "2021-09-02T21:03:48.846303Z"
        },
        "papermill": {
          "duration": 1.01264,
          "end_time": "2021-09-02T21:03:48.847271",
          "exception": false,
          "start_time": "2021-09-02T21:03:47.834631",
          "status": "completed"
        },
        "tags": [],
        "id": "double-thanks",
        "outputId": "c55a0b43-0b08-4d1b-f1ca-505461d2a969"
      },
      "source": [
        "df_train_ensemble2, df_test_ensemble2 = ensemble2.get_prediction_data()\n",
        "df_train_ensemble2"
      ],
      "id": "double-thanks",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>prediction_0</th>\n",
              "      <th>prediction_1</th>\n",
              "      <th>prediction_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>8.113634</td>\n",
              "      <td>8.450144</td>\n",
              "      <td>8.442234</td>\n",
              "      <td>8.460534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>8.481233</td>\n",
              "      <td>8.396575</td>\n",
              "      <td>8.401650</td>\n",
              "      <td>8.395044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>8.364351</td>\n",
              "      <td>8.210033</td>\n",
              "      <td>8.209859</td>\n",
              "      <td>8.207698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>8.049253</td>\n",
              "      <td>8.417482</td>\n",
              "      <td>8.418248</td>\n",
              "      <td>8.409137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>7.972260</td>\n",
              "      <td>8.220647</td>\n",
              "      <td>8.230699</td>\n",
              "      <td>8.219082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299995</th>\n",
              "      <td>499993</td>\n",
              "      <td>7.945605</td>\n",
              "      <td>8.313033</td>\n",
              "      <td>8.317646</td>\n",
              "      <td>8.328501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299996</th>\n",
              "      <td>499996</td>\n",
              "      <td>7.326118</td>\n",
              "      <td>7.679708</td>\n",
              "      <td>7.692613</td>\n",
              "      <td>7.645113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299997</th>\n",
              "      <td>499997</td>\n",
              "      <td>8.706755</td>\n",
              "      <td>8.256454</td>\n",
              "      <td>8.260073</td>\n",
              "      <td>8.257978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299998</th>\n",
              "      <td>499998</td>\n",
              "      <td>7.229569</td>\n",
              "      <td>7.941735</td>\n",
              "      <td>7.939393</td>\n",
              "      <td>7.934277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299999</th>\n",
              "      <td>499999</td>\n",
              "      <td>8.631146</td>\n",
              "      <td>8.303014</td>\n",
              "      <td>8.296975</td>\n",
              "      <td>8.305993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300000 rows  5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id    target  prediction_0  prediction_1  prediction_2\n",
              "0            1  8.113634      8.450144      8.442234      8.460534\n",
              "1            2  8.481233      8.396575      8.401650      8.395044\n",
              "2            3  8.364351      8.210033      8.209859      8.207698\n",
              "3            4  8.049253      8.417482      8.418248      8.409137\n",
              "4            6  7.972260      8.220647      8.230699      8.219082\n",
              "...        ...       ...           ...           ...           ...\n",
              "299995  499993  7.945605      8.313033      8.317646      8.328501\n",
              "299996  499996  7.326118      7.679708      7.692613      7.645113\n",
              "299997  499997  8.706755      8.256454      8.260073      8.257978\n",
              "299998  499998  7.229569      7.941735      7.939393      7.934277\n",
              "299999  499999  8.631146      8.303014      8.296975      8.305993\n",
              "\n",
              "[300000 rows x 5 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.037485,
          "end_time": "2021-09-02T21:03:48.924473",
          "exception": false,
          "start_time": "2021-09-02T21:03:48.886988",
          "status": "completed"
        },
        "tags": [],
        "id": "headed-spiritual"
      },
      "source": [
        "Just like before, let's use a transformer that allows the essential columns through and use a `Lasso` model to produce the final results."
      ],
      "id": "headed-spiritual"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T21:03:49.007310Z",
          "iopub.status.busy": "2021-09-02T21:03:49.006597Z",
          "iopub.status.idle": "2021-09-02T21:03:49.666820Z",
          "shell.execute_reply": "2021-09-02T21:03:49.668075Z"
        },
        "papermill": {
          "duration": 0.70564,
          "end_time": "2021-09-02T21:03:49.668334",
          "exception": false,
          "start_time": "2021-09-02T21:03:48.962694",
          "status": "completed"
        },
        "tags": [],
        "id": "lonely-cable",
        "outputId": "d668d875-7ce8-4318-ed95-5df67483e4dd"
      },
      "source": [
        "passthrough_preprocessor = compose.ColumnTransformer(\n",
        "    transformers=[(\n",
        "        \"passthrough\", \n",
        "        \"passthrough\", \n",
        "        [c for c in df_train_ensemble1.columns.tolist() if c.startswith(\"prediction\")]\n",
        "    )]\n",
        ")\n",
        "\n",
        "final_model = LassoModel(passthrough_preprocessor, model_params={\n",
        "    'alpha': 0.0001, \n",
        "    'max_iter': 20000\n",
        "})\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    df_train_ensemble2, \n",
        "    df_test_ensemble2, \n",
        "    folds=CROSS_VALIDATION_FOLDS\n",
        ")\n",
        "trainer.fit([final_model])"
      ],
      "id": "lonely-cable",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FOLD 0] Model 1 Score: 0.7169627780241739\n",
            "[FOLD 1] Model 1 Score: 0.7163733060987669\n",
            "[FOLD 2] Model 1 Score: 0.7156846459771987\n",
            "[FOLD 3] Model 1 Score: 0.7175379180522874\n",
            "[FOLD 4] Model 1 Score: 0.7217435344107982\n",
            "[FOLD 5] Model 1 Score: 0.71462950096099\n",
            "[FOLD 6] Model 1 Score: 0.7175654792322992\n",
            "[FOLD 7] Model 1 Score: 0.7188885988496246\n",
            "[FOLD 8] Model 1 Score: 0.7197768321371735\n",
            "[FOLD 9] Model 1 Score: 0.713071325968279\n",
            "\n",
            "Model 0 Overall Score: 0.7172233919711591\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7172233919711591]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.054401,
          "end_time": "2021-09-02T21:03:49.794891",
          "exception": false,
          "start_time": "2021-09-02T21:03:49.740490",
          "status": "completed"
        },
        "tags": [],
        "id": "fluid-prague"
      },
      "source": [
        "## Preparing the final submission\n",
        "\n",
        "We trained a single model to get the final results directly from `trainer3.predictions_test[0]`. \n",
        "\n",
        "This is an array with the prediction results of each one of the 10-fold models that we trained. We can compute the mean of these results and create the submission file."
      ],
      "id": "fluid-prague"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-02T21:03:49.879343Z",
          "iopub.status.busy": "2021-09-02T21:03:49.877757Z",
          "iopub.status.idle": "2021-09-02T21:03:50.767159Z",
          "shell.execute_reply": "2021-09-02T21:03:50.768302Z"
        },
        "papermill": {
          "duration": 0.935157,
          "end_time": "2021-09-02T21:03:50.768521",
          "exception": false,
          "start_time": "2021-09-02T21:03:49.833364",
          "status": "completed"
        },
        "tags": [],
        "id": "delayed-allergy"
      },
      "source": [
        "predictions = trainer.predictions_test[0]\n",
        "target = np.mean(np.column_stack(predictions), axis=1)\n",
        "output = pd.DataFrame({'id': df_test.id, 'target': target})\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "id": "delayed-allergy",
      "execution_count": null,
      "outputs": []
    }
  ]
}